---
title: "mini-project2"
author: "Peter Gao"
editor_options: 
  chunk_output_type: console
---
```{r}

mean1 <- 60
mean2 <- 40
truediff <- mean1 - mean2
sd1 <- 15
sd2 <- 25
sd_ratio <- sd1 / sd2
n1 <- 50
n2 <- 70
numsims <- 1000


samp1 <- rnorm(n1, mean1, sd1)
samp2 <- rnorm(n2, mean2, sd2)
result <- t.test(x = samp1, y = samp2, var.equal = TRUE)
lower <- result$conf.int[1]
upper <- result$conf.int[2]
contains_truediff <- (lower <= truediff & upper >= truediff)

sim_data <- data.frame(response = c(samp1, samp2),
                       group = c(rep("Group 1", n1), rep("Group 2", n2)))
library(ggplot2)
ggplot(sim_data, aes(x = response, y = group)) +
  geom_boxplot() +
  labs(title = "Boxplot of Simulated Data", x = "Value", y = "Group")


contains_truediff_vec <- vector("logical", numsims)
for (i in 1:numsims) {
  samp1 <- rnorm(n1, mean1, sd1)
  samp2 <- rnorm(n2, mean2, sd2)
  result <- t.test(x = samp1, y = samp2, var.equal = TRUE)
  lower <- result$conf.int[1]
  upper <- result$conf.int[2]
  contains_truediff_vec[i] <- (lower <= truediff & upper >= truediff)
}


coverage <- mean(contains_truediff_vec)
print(paste("Estimated Coverage Rate:", coverage))


```

I created two groups. Group 1: mean=60, standard deviation=15, sample size=50. Group 2: mean=40, standard deviation=25, sample size=70. I set the numsims=1000 to simulated the for loop for 1000 times to get the estimated coverage rate. The central rectangle in each group's boxplot represents the interquartile range, which contains the middle 50% of the data points. For example, in group 1, the IQR is from 55 to 75. The IQR of group 2 is from 30 to around 78. The line inside each box represents the median of each group. The median of group1 is 60, and the median for group2 is 40. The estimated coverage rate is around 0.967. Which means that when I repeating the sample simulation for 1000 times, approximately 96.7% of those intervals captured the true difference, which is slightly higher than the expected 95%. It means that the confidence intervals are slightly more conservative, meaning that they are perhaps wider than necessary to achieve a 95% converge rate. 


Adapt the simulation to examine confidence interval width rather than confidence interval coverage.
```{r}
mean1 <- 60
mean2 <- 40
truediff <- mean1 - mean2
sd1 <- 15
sd2 <- 25
sd_ratio <- sd1 / sd2
n1 <- 50
n2 <- 70
numsims <- 1000

calc_ci_width <- function(lower, upper) {
  return(upper - lower)
}

ci_widths <- numeric(numsims)

for (i in 1:numsims) {
  samp1 <- rnorm(n1, mean1, sd1)
  samp2 <- rnorm(n2, mean2, sd2)
  result <- t.test(x = samp1, y = samp2, var.equal = TRUE)
  lower <- result$conf.int[1]
  upper <- result$conf.int[2]
  ci_widths[i] <- calc_ci_width(lower, upper)
}


average_width <- mean(ci_widths)
median_width <- median(ci_widths)
sd_width <- sd(ci_widths)


print(paste("Average CI Width:", average_width))
print(paste("Median CI Width:", median_width))
print(paste("Standard Deviation of CI Width:", sd_width))

library(ggplot2)
ggplot(data.frame(ci_widths), aes(x = ci_widths)) + 
  geom_histogram(binwidth = 1) + 
  labs(title = "Distribution of Confidence Interval Widths", x = "CI Width", y = "Frequency")
```

This is a histogram showing the distribution of confidence interval widths. The distribution of the CI widths seems to be unimodal, which is only one peak. The peak suggests that the CI widths is around 15 to 16, indicating that the most common interval width in the simulation.  The widths of the confidence intervals seems to range from 13 to over 19. 